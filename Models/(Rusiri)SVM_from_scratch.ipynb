{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkA7b9_ZlMsV",
        "outputId": "ed0137c6-752b-48db-f937-f85a1109c5e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # for handling multi-dimensional array operation\n",
        "import pandas as pd  # for reading data from csv \n",
        "import statsmodels.api as sm  # for finding the p-value\n",
        "from sklearn.preprocessing import MinMaxScaler  # for normalization\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# >> FEATURE SELECTION << #\n",
        "def remove_correlated_features(X):\n",
        "    corr_threshold = 0.9\n",
        "    corr = X.corr()\n",
        "    drop_columns = np.full(corr.shape[0], False, dtype=bool)\n",
        "    for i in range(corr.shape[0]):\n",
        "        for j in range(i + 1, corr.shape[0]):\n",
        "            if corr.iloc[i, j] >= corr_threshold:\n",
        "                drop_columns[j] = True\n",
        "    columns_dropped = X.columns[drop_columns]\n",
        "    X.drop(columns_dropped, axis=1, inplace=True)\n",
        "    return columns_dropped\n",
        "def remove_less_significant_features(X, Y):\n",
        "    sl = 0.05\n",
        "    regression_ols = None\n",
        "    columns_dropped = np.array([])\n",
        "    for itr in range(0, len(X.columns)):\n",
        "        regression_ols = sm.OLS(Y, X).fit()\n",
        "        max_col = regression_ols.pvalues.idxmax()\n",
        "        max_val = regression_ols.pvalues.max()\n",
        "        if max_val > sl:\n",
        "            X.drop(max_col, axis='columns', inplace=True)\n",
        "            columns_dropped = np.append(columns_dropped, [max_col])\n",
        "        else:\n",
        "            break\n",
        "    regression_ols.summary()\n",
        "    return columns_dropped\n",
        "\n",
        "# >> MODEL TRAINING << #\n",
        "def compute_cost(W, X, Y):\n",
        "    # calculate hinge loss\n",
        "    N = X.shape[0]\n",
        "    distances = 1 - Y * (np.dot(X, W))\n",
        "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
        "    hinge_loss = reg_strength * (np.sum(distances) / N)\n",
        "    \n",
        "    # calculate cost\n",
        "    cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
        "    return cost\n",
        "\n",
        "# same function should work for vanilla and mini-batch gradient descent as well\n",
        "def calculate_cost_gradient(W, X_batch, Y_batch):\n",
        "    # if only one example is passed (eg. in case of SGD)\n",
        "    #print(type(Y_batch))\n",
        "    #print(X_batch)\n",
        "    if type(Y_batch) == np.int or type(Y_batch) == np.int64 or type(Y_batch) == np.float64 or type(Y_batch) == np.float :\n",
        "        Y_batch = np.array([Y_batch])\n",
        "        X_batch = np.array([X_batch])  # gives multidimensional array\n",
        "    \n",
        "    distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
        "    dw = np.zeros(len(W))\n",
        "\n",
        "    for ind, d in enumerate(distance):\n",
        "        if max(0, d) == 0:\n",
        "            di = W\n",
        "        else:\n",
        "            #print(Y_batch)\n",
        "            di = W - (reg_strength * Y_batch[ind] * X_batch[ind])\n",
        "        dw += di\n",
        "\n",
        "    dw = dw/len(Y_batch)  # average\n",
        "    return dw\n",
        "\n",
        "def sgd_1(features, outputs):\n",
        "    # running the loop 5000 times\n",
        "    max_epochs = 5000\n",
        "    weights = np.zeros(features.shape[1])\n",
        "    # stochastic gradient descent\n",
        "    for epoch in range(1, max_epochs): \n",
        "        # shuffle to prevent repeating update cycles\n",
        "        X, Y = shuffle(features, outputs)\n",
        "        for ind, x in enumerate(X):\n",
        "            ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
        "            weights = weights - (learning_rate * ascent)\n",
        "            \n",
        "    return weights\n",
        "\n",
        "#SGD with stoppage criterian\n",
        "def sgd(features, outputs):\n",
        "    #print(\"Features : \",features.shape)#620,5\n",
        "    #print(\"\\nOutputs : \",outputs.shape)#620,\n",
        "    max_epochs = 5000\n",
        "    weights = np.zeros(features.shape[1])\n",
        "    nth = 0\n",
        "    prev_cost = float(\"inf\")\n",
        "    cost_threshold = 0.01  # in percent\n",
        "    # stochastic gradient descent\n",
        "    for epoch in range(1, max_epochs):\n",
        "        # shuffle to prevent repeating update cycles\n",
        "        X, Y = shuffle(features, outputs)\n",
        "        #print([ind,x] for ind,x in enumerate(X))\n",
        "        for ind, x in enumerate(X):\n",
        "            #print(\"###\",str(x))\n",
        "            ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
        "            weights = weights - (learning_rate * ascent)\n",
        "        # convergence check on 2^nth epoch\n",
        "        if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
        "            cost = compute_cost(weights, features, outputs)\n",
        "            print(\"Epoch is:{} and Cost is: {}\".format(epoch, cost))\n",
        "            # stoppage criterion\n",
        "            if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
        "                return weights\n",
        "            prev_cost = cost\n",
        "            nth += 1\n",
        "    return weights\n",
        "\n",
        "def init():\n",
        "    df=pd.read_csv(\"drive/My Drive/Final Year Research/Dataset/Oneplus 5T/data_files/rgbarray.csv\")\n",
        "    labels = np.load(\"drive/My Drive/Final Year Research/Dataset/Oneplus 5T/data_files/labeldata.npy\")\n",
        "    metadata=pd.read_csv('drive/My Drive/Final Year Research/Dataset/Oneplus 5T/data_files/metadata.csv')\n",
        "    df['shutter_speed']=metadata['shutter_speed']\n",
        "    df['brightness']=metadata['brightness']\n",
        "    X = df.iloc[:, 0:]  # all rows of column 0 and ahead (features)\n",
        "    Y = pd.DataFrame(labels.astype(np.float64)).loc[:, 0]  # all rows of labels\n",
        "\n",
        "    # normalize the features using MinMaxScalar from\n",
        "    # sklearn.preprocessing\n",
        "    X_normalized = MinMaxScaler().fit_transform(X.values)\n",
        "    X = pd.DataFrame(X_normalized)\n",
        "\n",
        "    # first insert 1 in every row for intercept b\n",
        "    X.insert(loc=len(X.columns), column='intercept', value=1)\n",
        "\n",
        "    # test_size is the portion of data that will go into test set\n",
        "    # random_state is the seed used by the random number generator\n",
        "    print(\"splitting dataset into train and test sets...\")\n",
        "    X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # train the model\n",
        "    print(\"training started...\")\n",
        "    W = sgd(X_train.to_numpy(), y_train.to_numpy())\n",
        "    print(\"training finished.\")\n",
        "    print(\"weights are: {}\".format(W))\n",
        "\n",
        "    #After training the model using SGD we finally got the optimal weights w* which defines the best possible hyperplane separating two classes.\n",
        "\n",
        "    # testing the model on test set\n",
        "    y_test_predicted = np.array([])\n",
        "    #print(\"W : \",X_test.values[0])\n",
        "    for i in range(X_test.shape[0]):\n",
        "        print(i,\" : \",np.dot(W, X_test.values[i]))\n",
        "        yp = np.sign(np.dot(W, X_test.values[i])) #model\n",
        "        y_test_predicted = np.append(y_test_predicted, yp)\n",
        "    #print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test.values, y_test_predicted)))\n",
        "    #print(\"recall on test dataset: {}\".format(recall_score(y_test.values, y_test_predicted)))\n",
        "    #print(\"precision on test dataset: {}\".format(recall_score(y_test.values, y_test_predicted)))\n",
        "    #print(y_test.values)\n",
        "\n",
        "# set hyper-parameters and call init\n",
        "# hyper-parameters are normally tuned using cross-validation\n",
        "# but following work good enough\n",
        "reg_strength = 10000 # regularization strength\n",
        "learning_rate = 0.000001\n",
        "init()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V0oebNElSoK",
        "outputId": "8ce5aa2e-3ca0-47bd-a26c-4000dc5f2307"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "splitting dataset into train and test sets...\n",
            "training started...\n",
            "Epoch is:1 and Cost is: 0.07090397151665402\n",
            "Epoch is:2 and Cost is: 0.07081610503651643\n",
            "training finished.\n",
            "weights are: [0.15502772 0.16383188 0.09237367 0.12552984 0.11702145 0.22972392]\n",
            "0  :  0.6422899609874867\n",
            "1  :  0.5631766924815695\n",
            "2  :  0.6087721288384316\n",
            "3  :  0.6162188859185245\n",
            "4  :  0.6211655177197735\n",
            "5  :  0.6209665983868273\n",
            "6  :  0.5780275236783456\n",
            "7  :  0.5715971428738058\n",
            "8  :  0.4367751406143976\n",
            "9  :  0.6540138293642745\n",
            "10  :  0.6082806774951564\n",
            "11  :  0.3888610380963472\n",
            "12  :  0.47917719025590516\n",
            "13  :  0.47039047170495396\n",
            "14  :  0.6435288429685142\n",
            "15  :  0.5737980532973136\n",
            "16  :  0.7324747714284967\n",
            "17  :  0.42595559112916076\n",
            "18  :  0.5672312489830216\n",
            "19  :  0.43015287596853596\n",
            "20  :  0.4421191015245101\n",
            "21  :  0.7029420201934785\n",
            "22  :  0.4438973663810287\n",
            "23  :  0.5303149414694134\n",
            "24  :  0.4490787028850008\n",
            "25  :  0.596459757144898\n",
            "26  :  0.39311808875974297\n",
            "27  :  0.4230979850010213\n",
            "28  :  0.7444056095182701\n",
            "29  :  0.6247741486202092\n",
            "30  :  0.6828458793286665\n",
            "31  :  0.6090396518545256\n",
            "32  :  0.6029808617311462\n",
            "33  :  0.5952918074692805\n",
            "34  :  0.5972767468558215\n",
            "35  :  0.6554536037323719\n",
            "36  :  0.6761123670419084\n",
            "37  :  0.6329322950117717\n",
            "38  :  0.6804242772215449\n",
            "39  :  0.6149860796297473\n",
            "40  :  0.6385883924716849\n",
            "41  :  0.6113675895146788\n",
            "42  :  0.6322384440011444\n",
            "43  :  0.6609834544065868\n",
            "44  :  0.6433147019144901\n",
            "45  :  0.5499927133773101\n",
            "46  :  0.46363346515083825\n",
            "47  :  0.5977373883452917\n",
            "48  :  0.603551337252522\n",
            "49  :  0.680114925618458\n",
            "50  :  0.5817622104961669\n",
            "51  :  0.5980011021563025\n",
            "52  :  0.5655362138288932\n",
            "53  :  0.7259800659089664\n",
            "54  :  0.679006335102331\n",
            "55  :  0.6225958290041944\n",
            "56  :  0.6428476838105903\n",
            "57  :  0.5972848524867468\n",
            "58  :  0.5097052097327064\n",
            "59  :  0.5604445836249584\n",
            "60  :  0.5154165633853215\n",
            "61  :  0.5059526972787366\n",
            "62  :  0.46182939133528644\n",
            "63  :  0.47305155171255486\n",
            "64  :  0.5637116691966846\n",
            "65  :  0.6971330821005162\n",
            "66  :  0.5811734794364737\n",
            "67  :  0.5737062920899298\n",
            "68  :  0.5876106867547768\n",
            "69  :  0.6899052539702297\n",
            "70  :  0.569288442710215\n",
            "71  :  0.6138521792549284\n",
            "72  :  0.5784051563027212\n",
            "73  :  0.5673424634251754\n",
            "74  :  0.5748512612812607\n",
            "75  :  0.44597927424980266\n",
            "76  :  0.6769754093047644\n",
            "77  :  0.6867542164864924\n",
            "78  :  0.664649137782633\n",
            "79  :  0.4075124965808742\n",
            "80  :  0.6406063929213955\n",
            "81  :  0.6031670445306483\n",
            "82  :  0.5879427411708462\n",
            "83  :  0.6110378201770821\n",
            "84  :  0.6146799074022581\n",
            "85  :  0.6256270127668074\n",
            "86  :  0.5910142776097533\n",
            "87  :  0.6532116295751511\n",
            "88  :  0.6895879682571906\n",
            "89  :  0.693774803522989\n",
            "90  :  0.43867030106568744\n",
            "91  :  0.4429190930820728\n",
            "92  :  0.6026884987550516\n",
            "93  :  0.5570423050343862\n",
            "94  :  0.6301024282487868\n",
            "95  :  0.6942835680788293\n",
            "96  :  0.6599790297659957\n",
            "97  :  0.4995985499201856\n",
            "98  :  0.7206352815134962\n",
            "99  :  0.7161413102519939\n",
            "100  :  0.4245309321895341\n",
            "101  :  0.6421386332170325\n",
            "102  :  0.5991309469601649\n",
            "103  :  0.5923412193169104\n",
            "104  :  0.4147381268209336\n",
            "105  :  0.7071264739130527\n",
            "106  :  0.6473803090528627\n",
            "107  :  0.697921648816288\n",
            "108  :  0.5727895983753601\n",
            "109  :  0.47947179231901016\n",
            "110  :  0.6047753087608747\n",
            "111  :  0.3952936169903494\n",
            "112  :  0.6466437645646843\n",
            "113  :  0.7279523806750761\n",
            "114  :  0.4343188284776976\n",
            "115  :  0.5800089663596977\n",
            "116  :  0.6278656973192716\n",
            "117  :  0.5631529186655073\n",
            "118  :  0.4221340791864847\n",
            "119  :  0.6530773412852894\n",
            "120  :  0.6597586267338925\n",
            "121  :  0.6045528526308159\n",
            "122  :  0.5992788703981325\n",
            "123  :  0.5528696352135178\n",
            "124  :  0.5174397289055925\n",
            "125  :  0.5595608768372294\n",
            "126  :  0.6758346837594671\n",
            "127  :  0.4358275634443966\n",
            "128  :  0.6584988373920485\n",
            "129  :  0.6778059175118931\n",
            "130  :  0.6685192980705092\n",
            "131  :  0.4440386470440699\n",
            "132  :  0.5950475365279305\n",
            "133  :  0.46451269975141923\n",
            "134  :  0.6305653292143385\n",
            "135  :  0.6349500120026441\n",
            "136  :  0.5825129329761507\n",
            "137  :  0.4212623962982498\n",
            "138  :  0.6419242000869968\n",
            "139  :  0.6672897329697232\n",
            "140  :  0.5571545250411374\n",
            "141  :  0.5744779104713826\n",
            "142  :  0.6057877908833308\n",
            "143  :  0.6182465951307958\n",
            "144  :  0.5757113049255258\n",
            "145  :  0.5968673252565792\n",
            "146  :  0.6549269437619185\n",
            "147  :  0.6589440659588822\n",
            "148  :  0.574616061091028\n",
            "149  :  0.3774499469213086\n",
            "150  :  0.4173775588393214\n",
            "151  :  0.5931885080276874\n",
            "152  :  0.5697199343836682\n",
            "153  :  0.6151931356777408\n",
            "154  :  0.6483651958275276\n",
            "155  :  0.4486550636364481\n"
          ]
        }
      ]
    }
  ]
}